{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd3a8bb-8d38-498b-bd28-c4c0f5322d0e",
   "metadata": {},
   "source": [
    "# CM3070 Computer Science Final Project\n",
    "# 01 Data Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**  \n",
    "1. [Introduction](#introduction)  \n",
    "2. [Objectives](#objectives)  \n",
    "3. [Setup and dependencies](#setup-and-dependencies)  \n",
    "   - [Installing dependencies](#installing-dependencies)  \n",
    "   - [Setting up file paths and output directories](#file-paths-and-output-directories)    \n",
    "4. [Metadata preparation](#metadata-preparation)  \n",
    "   - [Setting up emotion maps and inclusion rules](#emotion-maps-and-inclusion-rules)\n",
    "   - [Collecting all relevant WAV files from the datasets](#collecting-all-relevant-WAV-files)\n",
    "   - [Extracting the features from a WAV file](#extracting-the-features)\n",
    "5. [Exporting the files](#exporting-the-files)\n",
    "   - [Exporting all the files in the dataframe](#exporting-all-the-files-in-the-dataframe)\n",
    "6. [Summary and next steps](#summary)\n",
    "7. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b1180",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "This notebook marks the first stage of a larger project on **Neural Style Transfer (NST) in Speech**, which explores the transformation of spoken audio to adopt the *emotions* of another sample. Inspired by visual style transfer techniques, the goal is to blend the *content* of one speech recording (e.g., neutral narration) with the *emotional tone* or *style* of another (e.g., happy, angry).\n",
    "\n",
    "To enable this, we use the **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)** [[1](#reference-1)] and **CREMA-D (Crowd Sourced Emotional Multimodal Actors Dataset)** [[2](#reference-2)] datasets, which are well-suited for prosody-based audio style transfer because they:\n",
    "- Contain professional-quality speech recordings by 24 actors\n",
    "- Cover a handful emotions with multiple intensities and lexical content\n",
    "- Include clean, noise-free recordings suitable for signal processing\n",
    "- Are publicly available for research use\n",
    "\n",
    "In this notebook, we begin the preprocessing pipeline by loading, parsing, and converting the audio files from these two datasets into .npy files - a format suitable for training neural models.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Objectives <a id=\"objectives\"></a>\n",
    "\n",
    "- Load the RAVDESS and CREMA-D  datasets\n",
    "- Filter all emotions with strong intensity (these strong differences will help the models better distinguish between different emotions)\n",
    "- Standardize all audio files (e.g., perform log compression, standardise frame rates, etc.)\n",
    "- Extract prosody features from the audio files (e.g. pitch, energy)\n",
    "- Save the outputs as .npy files in a structured folder format by emotion category\n",
    "\n",
    "This preprocessing ensures that our dataset is:\n",
    "- Clean, consistent, and normalized across all samples\n",
    "- Structured to support emotion-specific modeling and style transfer\n",
    "- Ready for downstream tasks such as classification and style transformation using neural networks\n",
    "\n",
    "The outputs from this notebook form the foundation for the classification models and style transfer techniques developed in later stages.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Setup and dependencies <a id=\"setup-and-dependencies\"></a>\n",
    "### Installing dependencies <a id=\"installing-dependencies\"></a>\n",
    "In this cell, we install the necessary Python packages for:\n",
    "\n",
    "- Audio processing and feature extraction (`librosa`)\n",
    "- Displaying progress bars during loops (`tqdm`)\n",
    "- Efficient numerical operations and array handling (`numpy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0691541",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa tqdm numpy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679563f1",
   "metadata": {},
   "source": [
    "### Setting up file paths and output directories <a id=\"file-paths-and-output-directories\"></a>\n",
    "We define paths to the CREMA-D and RAVDESS datasets, and the temporary metadata CSV, creating it if not found. This CSV will be used to collect all the relevant files that are to be further processed into .npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260843e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define base paths\n",
    "CREMAD_DATA_DIR = \"datasets/CREMA-D\"\n",
    "RAVDESS_DATA_DIR = \"datasets/RAVDESS/\"\n",
    "CSV_DIR = \"csv/\"\n",
    "\n",
    "# create root output directory if it doesn't exist\n",
    "os.makedirs(CSV_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdb4ae-295e-4719-a340-7731a17c1b55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Metadata preparation <a id=\"metadata-preparation\"></a>\n",
    "### Setting up emotion maps and inclusion rules<a id=\"emotion-maps-and-inclusion-rules\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c157bff7-5bf1-4ca4-94e4-41d23c9797b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CREMA-D -------- #\n",
    "# map CREMA-D emotion codes to labels\n",
    "cremad_emotion_map = {\n",
    "    \"NEU\": \"neutral\",\n",
    "    \"HAP\": \"happy\",\n",
    "    \"SAD\": \"sad\",\n",
    "    \"ANG\": \"angry\",\n",
    "    \"FEA\": \"fearful\",\n",
    "    \"DIS\": \"disgust\"\n",
    "}\n",
    "\n",
    "# precompute valid (actor, statement) pairs that have at least one HI non-neutral clip\n",
    "cremad_audio_path = os.path.join(CREMAD_DATA_DIR, \"AudioWAV\")\n",
    "valid_pairs = set()\n",
    "\n",
    "for fname in os.listdir(cremad_audio_path):\n",
    "    if not fname.endswith(\".wav\"):\n",
    "        continue\n",
    "    parts = fname.split(\"_\")\n",
    "    if len(parts) != 4:\n",
    "        continue\n",
    "    actor, statement, emotion_code, intensity_with_ext = parts\n",
    "    emotion_code = emotion_code.upper()\n",
    "    intensity = intensity_with_ext.replace(\".wav\", \"\").upper()\n",
    "    if emotion_code in cremad_emotion_map and emotion_code != \"NEU\" and intensity == \"HI\":\n",
    "        valid_pairs.add((actor, statement))\n",
    "\n",
    "# function to decide if a CREMA-D file should be included\n",
    "def should_include_cremad_file(emotion_code, intensity, fname=None):\n",
    "    if emotion_code not in cremad_emotion_map:  # make sure mapping exists\n",
    "        return False\n",
    "\n",
    "    if emotion_code == \"NEU\":\n",
    "        # only include neutral files if they match a valid actor+statement pair\n",
    "        if fname is None:\n",
    "            return False\n",
    "        parts = fname.split(\"_\")\n",
    "        if len(parts) != 4:\n",
    "            return False\n",
    "        actor, statement, _, _ = parts\n",
    "        return (actor, statement) in valid_pairs and intensity == \"XX\"\n",
    "    else:\n",
    "        # for other emotions, only keep high intensity\n",
    "        return intensity == \"HI\"\n",
    "\n",
    "\n",
    "# -------- RAVDESS -------- #\n",
    "# map RAVDESS emotion codes to labels\n",
    "ravdess_emotion_map = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\"\n",
    "}\n",
    "\n",
    "# function to decide if a RAVDESS file should be included\n",
    "def should_include_ravdess_file(modality, channel, emotion, intensity):\n",
    "    if modality != \"03\" or channel != \"01\":  # only speech, audio-only files\n",
    "        return False\n",
    "    if emotion not in ravdess_emotion_map:  # make sure mapping exists\n",
    "        return False\n",
    "    if emotion == \"01\" and intensity != \"01\":  # neutral must be normal\n",
    "        return False\n",
    "    if emotion != \"01\" and intensity != \"02\":  # others must be strong\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db2817-df5a-4b9c-8dd4-c9f32a429939",
   "metadata": {},
   "source": [
    "### Collecting all relevant WAV files from the datasets <a id=\"collecting-all-relevant-WAV-files\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fe6096-bd5c-4ecf-982b-5364c572e891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        filepath  dataset actor  emotion  \\\n",
      "0  datasets/CREMA-D\\AudioWAV\\1001_IEO_ANG_HI.wav  CREMA-D  1001    angry   \n",
      "1  datasets/CREMA-D\\AudioWAV\\1001_IEO_DIS_HI.wav  CREMA-D  1001  disgust   \n",
      "2  datasets/CREMA-D\\AudioWAV\\1001_IEO_FEA_HI.wav  CREMA-D  1001  fearful   \n",
      "3  datasets/CREMA-D\\AudioWAV\\1001_IEO_HAP_HI.wav  CREMA-D  1001    happy   \n",
      "4  datasets/CREMA-D\\AudioWAV\\1001_IEO_NEU_XX.wav  CREMA-D  1001  neutral   \n",
      "\n",
      "  intensity  \n",
      "0        HI  \n",
      "1        HI  \n",
      "2        HI  \n",
      "3        HI  \n",
      "4        XX  \n",
      "\n",
      "Total files collected: 1122\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata = []\n",
    "\n",
    "# -------- CREMA-D -------- #\n",
    "cremad_audio_path = os.path.join(CREMAD_DATA_DIR, \"AudioWAV\")\n",
    "\n",
    "for file in os.listdir(cremad_audio_path):\n",
    "    if not file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    # Example CREMA-D filename: 1001_DFA_ANG_HI.wav\n",
    "    parts = file.split(\"_\")\n",
    "    if len(parts) != 4:\n",
    "        continue\n",
    "\n",
    "    actor, statement, emotion_code, intensity_with_ext = parts\n",
    "    intensity = intensity_with_ext.replace(\".wav\", \"\")\n",
    "\n",
    "    # pass the filename so neutral files can be checked against valid_pairs\n",
    "    if should_include_cremad_file(emotion_code, intensity, fname=file):\n",
    "        label = cremad_emotion_map[emotion_code]\n",
    "        filepath = os.path.join(cremad_audio_path, file)\n",
    "        metadata.append({\n",
    "            \"filepath\": filepath,\n",
    "            \"dataset\": \"CREMA-D\",\n",
    "            \"actor\": actor,\n",
    "            \"emotion\": label,\n",
    "            \"intensity\": intensity\n",
    "        })\n",
    "\n",
    "# -------- RAVDESS -------- #\n",
    "for actor_dir in os.listdir(RAVDESS_DATA_DIR):\n",
    "    actor_path = os.path.join(RAVDESS_DATA_DIR, actor_dir)\n",
    "    if not os.path.isdir(actor_path):\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(actor_path):\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "            \n",
    "        # example RAVDESS filename: 03-01-01-01-01-01-01.wav\n",
    "        parts = file.split(\"-\")\n",
    "        if len(parts) != 7:\n",
    "            continue\n",
    "\n",
    "        modality, vocal_channel, emotion, intensity, statement, repetition, actor = parts\n",
    "\n",
    "        if should_include_ravdess_file(modality, vocal_channel, emotion, intensity):\n",
    "            label = ravdess_emotion_map[emotion]\n",
    "            filepath = os.path.join(actor_path, file)\n",
    "            metadata.append({\n",
    "                \"filepath\": filepath,\n",
    "                \"dataset\": \"RAVDESS\",\n",
    "                \"actor\": actor,\n",
    "                \"emotion\": label,\n",
    "                \"intensity\": intensity\n",
    "            })\n",
    "\n",
    "# save metadate onto a dataframe\n",
    "df = pd.DataFrame(metadata)\n",
    "print(df.head())\n",
    "print(\"\\nTotal files collected:\", len(df))\n",
    "\n",
    "# save metadata for later use\n",
    "df.to_csv(os.path.join(CSV_DIR, \"metadata.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8283f3-a8e3-4165-8a2d-42de91aac41c",
   "metadata": {},
   "source": [
    "### Extracting the features from a WAV file <a id=\"extracting-the-features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690451c5-6c3d-4fc1-b6cc-09e361b60966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# Settings\n",
    "# -----------------------------\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 80\n",
    "FIXED_FRAMES = 256  # standard length for time frames\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 256\n",
    "N_MFCC = 13  # number of MFCCs\n",
    "\n",
    "# -----------------------------\n",
    "# Utility functions\n",
    "# -----------------------------\n",
    "def normalize_audio(y):\n",
    "    \"\"\"Normalize waveform to [-1, 1].\"\"\"\n",
    "    return y / np.max(np.abs(y)) if np.max(np.abs(y)) > 0 else y\n",
    "\n",
    "def pad_or_truncate_audio(y, target_samples):\n",
    "    \"\"\"Pad or truncate waveform to target number of samples.\"\"\"\n",
    "    if len(y) > target_samples:\n",
    "        return y[:target_samples]\n",
    "    elif len(y) < target_samples:\n",
    "        return np.pad(y, (0, target_samples - len(y)), mode='constant')\n",
    "    return y\n",
    "\n",
    "def pad_or_truncate_spec(spec, target_frames=FIXED_FRAMES):\n",
    "    \"\"\"Pad or truncate 2D or 1D feature arrays to fixed time frames.\"\"\"\n",
    "    if spec.ndim == 2:\n",
    "        if spec.shape[1] > target_frames:\n",
    "            return spec[:, :target_frames]\n",
    "        elif spec.shape[1] < target_frames:\n",
    "            pad_width = target_frames - spec.shape[1]\n",
    "            return np.pad(spec, ((0, 0), (0, pad_width)), mode='constant', constant_values=1e-6)\n",
    "    elif spec.ndim == 1:\n",
    "        if len(spec) > target_frames:\n",
    "            return spec[:target_frames]\n",
    "        elif len(spec) < target_frames:\n",
    "            pad_width = target_frames - len(spec)\n",
    "            return np.pad(spec, (0, pad_width), mode='constant', constant_values=1e-6)\n",
    "    return spec\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extraction functions\n",
    "# -----------------------------\n",
    "def extract_mel_spectrogram(y, sr, n_mels=N_MELS, target_frames=FIXED_FRAMES):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=n_mels,\n",
    "        n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "        power=1.0\n",
    "    )\n",
    "    mel = np.log(mel + 1e-9)\n",
    "    mel = pad_or_truncate_spec(mel, target_frames)\n",
    "    return mel.astype(np.float32)\n",
    "\n",
    "def extract_mfcc(y, sr, n_mfcc=N_MFCC, target_frames=FIXED_FRAMES):\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y, sr=sr, n_mfcc=n_mfcc,\n",
    "        n_fft=N_FFT, hop_length=HOP_LENGTH\n",
    "    )\n",
    "    # Compute delta and delta-delta\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    mfcc_features = np.vstack([mfcc, mfcc_delta, mfcc_delta2])\n",
    "    mfcc_features = pad_or_truncate_spec(mfcc_features, target_frames)\n",
    "    return mfcc_features.astype(np.float32)\n",
    "\n",
    "def extract_pitch(y, sr, target_frames=FIXED_FRAMES):\n",
    "    f0, _, _ = librosa.pyin(\n",
    "        y, fmin=librosa.note_to_hz('C2'),\n",
    "        fmax=librosa.note_to_hz('C7')\n",
    "    )\n",
    "    f0 = np.nan_to_num(f0)\n",
    "    f0 = pad_or_truncate_spec(f0, target_frames)\n",
    "    return f0.astype(np.float32)\n",
    "\n",
    "def extract_energy(y, frame_length=2048, hop_length=512, target_frames=FIXED_FRAMES):\n",
    "    energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "    energy = pad_or_truncate_spec(energy, target_frames)\n",
    "    return energy.astype(np.float32)\n",
    "\n",
    "def extract_spectral_features(y, sr, target_frames=FIXED_FRAMES):\n",
    "    # Spectral centroid\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)[0]\n",
    "    centroid = pad_or_truncate_spec(centroid, target_frames)\n",
    "\n",
    "    # Spectral bandwidth\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)[0]\n",
    "    bandwidth = pad_or_truncate_spec(bandwidth, target_frames)\n",
    "\n",
    "    # Spectral contrast\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    contrast = pad_or_truncate_spec(contrast, target_frames)\n",
    "\n",
    "    spectral_feats = np.vstack([centroid, bandwidth, contrast])\n",
    "    return spectral_feats.astype(np.float32)\n",
    "\n",
    "def extract_duration(y, sr):\n",
    "    return len(y) / sr\n",
    "\n",
    "# -----------------------------\n",
    "# Main processing function\n",
    "# -----------------------------\n",
    "def process_and_save_features(file_path, output_dir, emotion_label,\n",
    "                              sample_rate=SAMPLE_RATE, n_mels=N_MELS, target_frames=FIXED_FRAMES):\n",
    "    # Load and normalize waveform\n",
    "    y, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    y = normalize_audio(y)\n",
    "\n",
    "    # Pad waveform to ensure enough frames\n",
    "    target_samples = (target_frames - 1) * HOP_LENGTH + N_FFT\n",
    "    y = pad_or_truncate_audio(y, target_samples)\n",
    "\n",
    "    # Extract features\n",
    "    mel_db = extract_mel_spectrogram(y, sr, n_mels, target_frames)\n",
    "    mfcc_feats = extract_mfcc(y, sr, N_MFCC, target_frames)\n",
    "    f0 = extract_pitch(y, sr, target_frames)\n",
    "    energy = extract_energy(y, target_frames=target_frames)\n",
    "    spectral_feats = extract_spectral_features(y, sr, target_frames)\n",
    "    duration = extract_duration(y, sr)\n",
    "\n",
    "    # Save features\n",
    "    features = {\n",
    "        \"mel\": mel_db,\n",
    "        \"mfcc\": mfcc_feats,\n",
    "        \"f0\": f0,\n",
    "        \"energy\": energy,\n",
    "        \"spectral\": spectral_feats,\n",
    "        \"duration\": np.float32(duration),\n",
    "        \"sr\": np.int32(sr),\n",
    "        \"emotion\": emotion_label\n",
    "    }\n",
    "\n",
    "    out_dir = os.path.join(output_dir, emotion_label)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    file_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    out_path = os.path.join(out_dir, f\"{file_id}.npy\")\n",
    "    np.save(out_path, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7409a-b06b-43a7-ad1a-476c924bbf9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Exporting the files <a id=\"exporting-the-files\"></a>\n",
    "### Exporting all the files in the dataframe<a id=\"exporting-all-the-files-in-the-dataframe\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecdba1f9-9457-48b1-b35c-45a5191fc5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1122/1122 [12:02<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(df, output_dir=\"npys\"):\n",
    "    \"\"\"\n",
    "    Given df with columns [filepath, emotion],\n",
    "    extract and save features as .npy files.\n",
    "    \"\"\"\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        process_and_save_features(row[\"filepath\"], output_dir, row[\"emotion\"])\n",
    "\n",
    "preprocess_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a9b7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Summary and next steps <a id=\"summary\"></a>\n",
    "### Summary\n",
    "The relevant audio files have been exported as .npy files and are now organized into the folder:\n",
    "- `npys/`\n",
    "\n",
    "which each contain 6 emotion sub-folders:\n",
    "- `angry`\n",
    "- `disgust`\n",
    "- `fearful`\n",
    "- `happy`\n",
    "- `neutral`\n",
    "- `sad`\n",
    "\n",
    "### Next steps\n",
    "We will use these .npy files as inputs to train a neural network for emotion classification, to help identify different emotions.\n",
    "After that, we will use the prosody features captured (pitch, energy) within the .npy files to help transform a neutral input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadc5de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. References <a id=\"references\"></a>\n",
    "[1] <a id=\"reference-1\"></a> Affective Data Science Lab (ADSL), 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) [Online] \n",
    "Available at: https://zenodo.org/records/1188976\n",
    "[Accessed 8 June 2025].\n",
    "\n",
    "\n",
    "[2] <a id=\"reference-2\"></a>  Cao, H., Cooper, D.G., Keutmann, M.K., Gur, R.C., Nenkova, A. & Verma, R., 2014. CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset. [Online] \n",
    "Available at: https://pmc.ncbi.nlm.nih.gov/articles/PMC4313618/\n",
    "[Accessed 8 August 2025]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
